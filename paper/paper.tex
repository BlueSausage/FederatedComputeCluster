\documentclass{sop}
\usepackage[ngerman]{babel}
\usepackage{lipsum}
\usepackage{amsmath,amsfonts,amssymb}
\usepackage{tikz, graphics, subcaption}
\usepackage{tcolorbox}
\usepackage{graphicx}
\usepackage{fancyhdr}
\usepackage{hyperref}

\voffset 0 cm \hoffset 0 cm \addtolength{\textwidth}{0cm}
\addtolength{\textheight}{0cm}\addtolength{\leftmargin}{0cm}

\begin{document}

\title{Lernverhalten und Gewinnverteilung im Multi-Agenten-Markt für Auftragsvergabe mit Q-Learning}

%***********************************************************************
% AUTHORS INFORMATION AREA
%***********************************************************************
\author{Niklas Schneider
%
% DO NOT MODIFY THE FOLLOWING '\vspace' ARGUMENT
\vspace{.3cm}\\
%
HAW Hamburg - Department Informatik \\
Berliner Tor 5, 20099 Hamburg \\
niklas.schneider@haw-hamburg.de
}
%***********************************************************************
% END OF AUTHORS INFORMATION AREA
%***********************************************************************

\maketitle

\begin{abstract}
In dieser Arbeit wird ein dezentraler Markt zur Auftragsvergabe zwischen Rechenzentren (RZs) modelliert und mithilfe tabellarischen Q-Learnings analysiert. Jedes RZ agiert als autonomer Agent, der bei dynamisch generierten Auftragskosten und -werten zwischen Selbstbearbeitung, Outsourcing und Bietstrategien wählen kann. Ziel ist es, das Lernverhalten der Agenten sowie die langfristige Verteilung von Gewinnen und Aufträgen zu untersuchen. Zwei Experimente, eines mit dynamischen, eines mit festen Bearbeitungskosten, zeigen, dass Agenten unter geeigneter Modellierung stabile und ökonomisch sinnvolle Strategien entwickeln. Während sich im dynamischen Szenario langfristig eine faire Ergebnisverteilung einstellt, setzen sich im statischen Fall Agenten mit strukturellen Vorteilen durch, sofern sie diese auch strategisch nutzen. Die Ergebnisse unterstreichen die Bedeutung von Lernen und Strategieanpassung in komplexen Multi-Agenten-Märkten.
\end{abstract}

\section{Einführung}
Im Rahmen des Praktikum zu verteilten adaptiven Systemen wurde die Auftragsvergabe zwischen Rechenzentren (RZs) bisher zentral oder regelbasiert organisiert. In dieser Arbeit wird das Szenario zu einem dezentralen Markt weiterentwickelt, in dem mehrere RZs als autonome, lernende Agenten agieren. Sie können Aufträge selbstbearbeiten, auf dem Markt anbieten oder mit Erlösen aus der Selbstbearbeitung auf fremde Aufträge bieten. Ziel ist es, mit Hilfe von Q-Learning zu untersuchen, wie sich Strategien, Gewinne und die Verteilung der Aufträge unter realistischen Marktbedingungen entwickeln.

\subsection{Preisgestaltung} \label{price}
Zur Modellierung eines realitätsnahen Marktumfelds werden zwei verschiedene Varianten der Preis- und Kostenstruktur betrachtet. Eine dynamische, rundenweise variierende Variante sowie ein statisches, festes Kostenmodell für die Agenten.
\newline
Im dynamischen Szenario werden sowohl die Bearbeitungskosten der RZs als auch der Auftragswert in jeder Runde neu generiert. Jedes RZ zieht zu Beginn jeder Runde seine individuellen Bearbeitungskosten aus einer Normalverteilung $N(\mu_{cost}, \sigma_{cost})$ mit einem Erwartungswert $\mu_{cost}$ von 5 und einer Standardabweichung $\sigma_{cost}$ von 2. Die gezogenen Werte werden auf positive ganze Zahlen abgerundet und auf einen Wertebereich von 1 bis 10 begrenzt. Diese Begrenzung erleichtert die anschließende Diskretisierung der Zustände.
Die Wahl von $\mu = 5$ orientiert sich an einem mittleren Kostenniveau, das zugleich als Referenzwert für die Auftragsbewertung dient. Die Standardabweichung $\sigma = 2$ sorgt für eine realistische Streuung der Kosten um diesen Mittelwert. Nach der 68-95-99.7-Regel liegen damit ca. 95\% der gezogenen Werte im Intervall $[1, 9]$, was in Kombination mit der Begrenzung auf $[1, 10]$ einen kontrollierten, aber variierenden Kostenbereich gewährleistet \cite{westfallunderstanding}. So wird eine glaubwürdige Marktunsicherheit erzeugt, bei der Agenten sowohl in profitablen als auch in verlustreichen Situationen Entscheidungen treffen müssen.
\newline
Der Preis, der in einer Runde für alle Aufträge gleichermaßen gilt, wird als der aufgerundete Erwartungswert der gezogenen Kosten aller RZs berechnet. Dadurch ist der Auftragswert immer mindestens so hoch wie der Durchschnitt der Bearbeitungskosten im System. Diese dynamische Preisgestaltung erzeugt ein spannungsreiches Umfeld, in dem RZs je nach individueller Kostensituation entweder Gewinn, Verlust oder Break-Even erzielen können. RZs mit überdurchschnittlich hohen Kosten haben die Möglichkeit, ihre Aufträge am Markt zu verkaufen, während günstigere RZs durch Zukäufe zusätzliche Gewinne erzielen können.
\newline
Im statischen Szenario hingegen erhalten die RZs feste Bearbeitungskosten, die sich über alle Runden nicht verändern. In den durchgeführten Experimenten wurden folgende Kostenwerte verwendet: $[2, 3, 5, 7, 8]$. Diese decken systematisch alle Zustände des Modells ab, da der rundenweise berechnete Preis auch hier auf Basis des Durchschnitts der Agentenkosten entsteht. Anders als im dynamischen Fall verbleiben die Kostenstrukturen jedoch über alle Runden konstant, was es ermöglicht, strukturelle Vorteile einzelner Agenten gezielt zu untersuchen.
\newline
Beide Varianten dienen dazu, unterschiedliche Aspekte des Agentenverhaltens und der Ergebnisverteilung im Markt zu beleuchten und ermöglichen einen fundierten Vergleich zwischen Lernszenarien mit Unsicherheit und solchen mit festen Bedingungen.

\subsection{Auktionsverfahren}
In dem betrachteten Marktszenario haben RZs in jeder Runde drei Handlungs-möglichkeiten. Sie können ihren Auftrag selbst bearbeiten, ihn auf dem Markt zum Verkauf anbieten oder mit dem Erlös aus der Selbstbearbeitung ein Gebot für einen weiteren Auftrag abgeben.
\newline
Wird ein Auftrag auf dem Markt angeboten, so erfolgt dies ohne eines Mindestgebots. Alle angeboten Aufträge werden gleich behandelt und ohne individuelle Preisuntergrenze gelistet. Die RZs, die ein Gebot abgeben möchten, wählen dazu einen Betrag aus, den sie bereit sind, für die Bearbeitung eines fremden Auftrags zu zahlen. Die Höhe des Gebots orientiert sich an dem Gewinn aus der Selbstbearbeitung in der laufenden Runde, wobei feste Gebotsfaktoren verwendet werden (vgl Abschnitt \ref{actionspace} Aktionsraum).
\newline
Nachdem alle RZs ihre Entscheidung getroffen haben, erfolgt das Matching der Marktangebote. Die auf dem Markt gelisteten Aufträge werden zufällig einem der Bieter zugewiesen, wobei in absteigender Reihenfolge der Gebote vorgegangen wird. Jedes Angebot kann dabei maximal einmal vergeben werden. Es ist somit möglich, dass RZs, die ihren Auftrag auf dem Markt anbieten, keinen Käufer finden. Ebenso konnen RZs, die ein Gebot abgeben, leer ausgehen, wenn kein passender Auftrag mehr verfügbar ist.
\newline
Dieses einfache, nicht-kooperative Marktverfahren erschafft einen Auktionsmechanismus mit begrenztem Wettbewerb und Unsicherheit, der des den Agenten erlaubt, durch wiederholte Teilnahme und Lernen über Q-Learning geeignete Strategien zu entwickeln.

\section{Forschungsfrage}
Im Zentrum der Arbeit steht die Frage, wie sich das Verhalten und die Ergebnisse im beschriebenen Marktszenario entwickeln, wenn mehrere RZs als autonome Agenten mithilfe von Q-Learning agieren. Dabei werden insbesondere die Stabilität der Strategien, die Herausbildung von Gewinnern und die Verteilung der Aufträge analysiert.
\\
Daraus ergeben sich die folgenden Forschungsfragen:
\begin{enumerate}
    \item Entwickeln Agenten im Multi-Agenten-Markt für Auftragsvergabe durch wiederholte Teilnahme stabile und vorteilhafte Strategien?
    \item Verteilen sich Gewinne und Aufträge langfristig gleichmäßig, oder setzen sich einzelne Agenten als Gewinner durch?
\end{enumerate}

Um diese Fragen zu beantwortung wird eine simulationsbasierte Analyse durchgeführt. Dazu wird eine Multi-Agenten-Simulation entwickelt, in der mehrere RZs als autonome Agenten mithilfe von Q-Learning im Markt agieren. Die Agenten verfügen dabei jeweils nur über Informationen zu ihren eigenen Kosten und zur Gebühr für die Bearbeitung eines Auftrags, sowie die Marktauslastung der letzten Runde. Untersucht werden sowohl Szenarien mit festen Kosten je Agent als auch mit dynamischen, pro Runde neu gezogenen Kosten. Dieses Vorgehen ermöglicht es, das Zusammenspiel von individuellen Lernprozessen, Strategieentwicklung und Systemdynamik im dezentralen Markt direkt zu beobachten. Analytische Lösungen sind bei komplexem Agentenverhalten und Lernalgorithmen oft nicht praktikabel. Daher bietet die simulationsbasierte Auswertung eine realistische Möglichkeit, die Forschungsfragen systematisch zu untersuchen.

\section{Methodik/Grundlagen}
\subsection{Q-Learning}
Q-Learning ist ein modellfreies, off-policy Verfahren des Reinforcement Learning, das darauf abzielt, direkt die optimale Aktionswertfunktion $Q^*$ und damit eine optimale Strategie (Policy) $\pi^*$ zu erlernen, dabei kann der Agent einer explorativen Policy folgen, während das Lernen unabhängig davon erfolgt. Zentral ist die Q-Funktion $Q(s, a)$, die den erwarteten kumulierten Ertrag angibt, wenn der Agent in Zustand $s$ die Aktion $a$ wählt und anschließend optimal handelt \cite{sutton}. Das Update basiert auf der maximalen Bewertung des Folgezustands und erfordert kein Modell der Umwelt \cite{busoniucomprehensive}. 
\newline
Die optimale Q-Funktion $Q^*$ erfüllt die Bellman-Optimalitätsgleichung und beschreibt den erwarteten kumulierten Ertrag, wenn der Agent sich in einem Zustand $s$ befindet, die Aktion $a$ ausführt und anschließend optimal handelt \cite{szepesvarireinforcement}.
\[
Q^*(s, a) = \mathbb{E} \left[ R_{t+1} + \gamma  \max_{a'} Q^*(s', a') \vert S_t=s, A_t=a \right]
\]
wobei der Erwartungswert sowohl die stochastische Belohnung als auch die Übergangsdynamik der Umgebung berücksichtigt \cite{mnihplaying}.

Da die Übergangswahrscheinlichkeit und Belohnungsverteilung in modelfreien Verfahren nicht bekannt sind, wird die Aktionswertfunktion $Q(s, a)$ iterativ durch Interaktionen mit der Umgebung geschätzt \cite{busoniucomprehensive}. Beim Q-Learning erfolgt die Aktualisierung der Q-Werte schrittweise auf Basis eines temporalen Differenzfehlers \cite{busoniucomprehensive}. Nach der Ausführung einer Aktion $a$ im Zustand $s$, dem Beobachten der Belohnung $R_{t+1}$ und des Folgezustands $s'$,wird der zugehörige Q-Wert mithilfe der folgenden Update-Regel angepasst \cite{sutton}
\[
Q(s, a) \leftarrow Q(s, a) + \alpha (R_{t+1} + \gamma \max_{a'}Q(s', a') - Q(s, a))
\]
\newline
Hierbei steht $\alpha$ für die Lernrate, die bestimmt, wie stark neue Informationen bestehende Schätzungen beeinflussen, und $\gamma$ ist der Diskontierungsfaktor, der den Einfluss zukünftiger Belohnungen gewichtet. Dadurch handelt es sich um ein Off-Policy-Verfahren, das direkt gegen die optimale Aktionswertfunktion $Q^*$ konvergiert, auch wenn der Agent während des Lernens einer explorativen Strategie folgt \cite{sutton}.
\newline
\newline
Um ein Gleichgewicht zwischen dem Ausprobieren neuer Aktionen und der Nutzung bereits bekannter, vielversprechender Aktionen zu erreichen, wird in der Praxis häufig eine $\epsilon$-greedy-Policy eingesetzt \cite{szepesvarireinforcement}. Mit Wahrscheinlichkeit $\epsilon$ wählt der Agent eine zufällige Aktion (Exploration), andernfalls entscheidet sich der Agent mit der Wahrscheinlichkeit $1-\epsilon$ für die aktuell beste bekannte Aktion (Exploitation). Auf diese Weise wird Exploration ermöglicht, während gleichzeitig zunehmend bessere Strategien ausgenutzt werden. Unter geeigneten Bedingungen, insbesondere wenn alle Zustands-Aktions-Paare hinreichend oft besucht werden, kann Q-Learning so mit hoher Wahrscheinlichkeit zur optimalen Strategie konvergieren \cite{busoniucomprehensive}.

\subsection{Explodierende State und Action Spaces} \label{explodingstateactionspaces}
Beim Einsatz von Q-Learning in Multi-Agenten-Systemen führt die Kombination der Zustände und Aktionen aller Agenten zu einem exponentiell wachsenden Zustands- und Aktionsraum \cite{busoniucomprehensive, tuylsmultiagent}. Da Lernen im Kern als Suchprozess verstanden werden kann, wächst die Komplexität des Lernproblems mit der Anzahl beteiligter Variablen schnell an. Besonders im Multi-Agenten-Setting, in dem mehrere Agenten gleichzeitig agieren und interagieren, entstehen folgende Herausforderungen:
\begin{itemize}
    \item \textbf{Zustandsraum-Explosion:} Der Zustandsraum wächst exponentiell mit der Anzahl der Agenten und beobachtbaren Merkmale der Umgebung.
    \item \textbf{Aktionsraum-Explosion:} Ebenso wächst der gemeinsame Aktionsraum, da potenzielle Aktionskombinationen aller Agenten berücksichtigt werden müssen.
    \item \textbf{Ergebnisraum-Explosion:} Zusätzlich steigt die Zahl möglicher Ergebniszustände, also jener Zustände, die durch gleichzeitiges Handeln mehrerer Agenten erreicht werden können.
\end{itemize}
Diese Phänomene können dazu führen, dass klassische Lernverfahren wie Q-Learning schnell an praktische Grenzen stoßen, etwa, wenn der Speicher- oder Zeitaufwand für die Pflege der Q-Tabelle unhandhabbar wird. Um dieses Problem zu vermeiden, ist eine geeignete Reduktion und Diskretisierung der Zustände und Aktionen notwendig. \cite{tuylsmultiagent}

\subsection{Anwendung im Markt-Szenario}
Um das zuvor in \ref{explodingstateactionspaces} beschriebene Problem der Zustands- und Aktionsraumexplosion zu vermeiden, wurde das Markt-Szenario bewusst so modelliert, dass alle relevanten Elemente für tabellarisches Q-Learning diskret und handhabbar bleiben.
\newline
\newline
\textbf{Zustandsraum}\newline
Der Zustand eines Agenten setzt sich aus zwei diskretisierten Merkmalen zusammen:
\begin{enumerate}
    \item \textbf{Profitabilitätsniveau} - eine Einschätzung, ob der aktuelle Auftrag Verlust, Break-Even oder Gewinn bedeutet. Hierfür werden die individuell gezogenen Kosten mit dem aktuellen Auftragswert verglichen und in drei Kategorien eingeteilt:
    \begin{enumerate}
        \item[] 0: Verlust (Kosten $>$ Auftragswert)
        \item[] 1: Break-Even (Kosten $=$ Auftragswert)
        \item[] 2: Gewinn (Kosten $<$ Auftragswert)
    \end{enumerate}
    \item \textbf{Marktauslastung der Vorherigen Runden} - um die Auslastung des Marktes im vorherigen Runden zu erfassen, wird der Marktzustand nicht nur anhand eines einzelnen Preissignals bestimmt, sondern über einen kombinierten Marktindikator, der sowohl Preis- als auch Volumeninformationen berücksichtigt.\newline
    Da alle Agenten gleichzeitig in einer Runde agieren, existiert der aktuelle Markt zum Zeitpunkt der Entscheidung noch nicht. Daher stützt sich der Agent auf die Marktaktivität der vorherigen Runden.\newline
    In jeder Runde wird berechnet, wie viel die erfolgreichen Bieter im Durchschnitt zahlen mussten, um einen zusätzlichen Auftrag zu erhalten. Der durchschnittlich gezahlte Preis in Runde $t$ ergibt sich als Mittelwert aller erfolgreichen Gebote
    \[\bar{p}_t = \frac{1}{|W_t|} \sum_{b \in W_t} b \text{ für } |W_t| > 0\]
    Wobei $W_t$ die Menge der in Runde $t$ erfolgreichen Gebote bezeichnet und $b$ für die Höhe der jeweiligen Gebote (für $|W_t| = 0$ wird $\bar{p}_t = 0$ gesetzt)\newline
    Zusätztlich wird erfasst, wie viele Aufträge in einer Runde tatsächlich über den Markt vergeben wurden
    \[v_t = |W_t|\]
    Preis- und Volumeninformationen werden zu einem gemeinsamen Marktauslastungswert kombiniert. Dazu wird der durchschnittlich gezahlte Preis mit einem relativen Handelsvolumen multipliziert, das sich aus dem Verhältnis der erfolgreichen Handel zur maximal möglichen Anzahl an Transfers $N$ (Anzahl der Agenten) ergibt
    \[m_t = \bar{p}_t \cdot \frac{v_t}{N}\]
    Um zufällige Schwankungen einzelner Runden zu reduzieren, wird der Marktauslastungswert über ein gleitendes Fenster der letzten Runden gemittelt.\newline
    Die Diskretisierung erfolgt relativ zur eigenen Markthistorie. Hierzu wird der Marktauslastungswert der vergangenen Runden anhand des 33\%- und 66\%-Perzentils in drei Breiche unterteilt:
    \begin{enumerate}
        \item[] 0: niedriger Marktdruck (unter dem 33\%-Perzentil)
        \item[] 1: mittlerer Marktdruck (zwischen 33\% und 66\%-Perzentil)
        \item[] 2: hoher Marktdruck (über dem 66\%-Perzentil)
    \end{enumerate}
\end{enumerate}
Die Kombination dieser beiden Merkmale ergibt einen Zustandsraum von $3 \times 3 = 9$ diskreten Zuständen pro Agent.
\newline
\newline
\textbf{Aktionsraum} \label{actionspace}\newline
Um den Anforderungen von tabellarischem Q-Learning gerecht zu werden, wurde der Aktionsraum als festes, endliches Set diskreter Entscheidungen modelliert. Der Agent kann in jeder Runde aus folgenden sechs Aktionen wählen:
\begin{enumerate}
    \item [] Aktion 0: Eigenen Auftrag auf dem Markt anbieten
    \item [] Aktion 1: Auftrag selbst bearbeiten
    \item [] Aktion 2-5: Auf fremde Aufträge bieten, wobei das Gebot einem festen vielfachen $F \in \{0.25, 0.5, 0.75, 1.0\}$ des Erlöses entspricht, den der Agent durch die Selbstbearbeitung erzielen würde.
\end{enumerate}
Die Entscheidung zum Bieten wurde auf vier diskrete Bietstufen beschränkt, um den kontinuierlichen Aktionsraum kompakt und lernbar zu halten. Damit ergibt sich der folgende diskrete Aktionsraum:
\[
A = \{0, 1, 2, 3, 4, 5\}
\]
\newline
\textbf{Belohnungsstruktur}\newline
Die Belohnung eines Agenten basiert auf dem ökonomischen Ergebnis seiner Aktion in der jeweiligen Runde. Im Erfolgsfall entspricht die Belohnung dem tatsächlichen Gewinn, also dem Erlös aus dem erhaltenen Auftrag abzüglich der Bearbeitungskosten. Wird ein eigener Auftrag selbst bearbeitet oder erfolgreich auf dem Markt verkauft bzw. ersteigert und anschließend bearbeitet, ergibt sich eine positive Belohnung, sofern Gewinn erzielt wird.
\newline
Um jedoch unrealistisches Verhalten zu verhindern, erhält der Agent eine explizite Strafbelohnung von -10, wenn er eine Bietaktion (Aktion 2-5) auswählt, obwohl in dieser Runde kein Gewinn durch Selbstbearbeitung hätte erzielen können. Auf diese Weise wird vermieden, dass Agenten systematisch Gebote abgeben, obwohl ihnen dafür keine
wirtschaftliche Grundlage zur verfügung steht.
\newline
Nicht erfolgreiche Aktionen (z.B. kein Zuschlag bei Geboten oder keine Abnahme des angebotenen Aufztrags) führen zu einer Belohnung wie bei der Selbstbearbeitung.
\newline
\newline
Durch diese diskrete Modellierung ist es möglich, klassisches tabellarisches Q-Learning auch in einem dezentralen Multi-Agenten-Markt anzuwenden, ohne dass der Zustands- und Aktionsraum unhandhabbar groß wird.


\section{Experimente}
Das entwickelte Marktmodell wird mithilfe einer simulationsbasierten Multi-Agenten-Umgebung untersucht, in der RZs mittels Q-Learning handeln. Ziel ist es, das Lernverhalten der Agenten sowie die langfristige Verteilung von Aufträgen und Gewinnen unter verschiedenen Kostenbedingungen zu analysieren.

\subsection{Untersuchungsdesign}
Zur Untersuchung des Lernverhaltens und der Ergebnisverteilung im Marktmodell wurden zwei Simulationsexperimente mit jeweils fünf Agenten durchgeführt. Alle Agenten nutzen tabellarisches Q-Learning zur Entscheidung über ihre Aktionen. Jede Simulation erstreckt sich über 8000 Runden, basierend auf vorherigen Beobachtungen, dass sich der durchschnittliche Reward ab etwa Runde 3000 stabilisiert (siehe Abbildung \ref{fig:social_welfare}). Dadurch ist gewährleistet, dass genügend Lernzeit zur Verfügung steht, um langfristige Verhaltensmuster zu analysieren.
\newline
\newline
Im ersten Experiment werden die Agentenkosten dynamisch modelliert. In jeder Runde zieht jeder Agent seine individuellen Kosten aus einer Normalverteilung mit Erwartungswert $\mu_{cost} = 5$ und Standardabweichung $\sigma_{cost} = 2$ (siehe Kapitel \ref{price}). Durch diese Form der Zufallsvariation ergeben sich wechselnde Marktbedingungen, unter denen alle Agenten theoretisch gleich gute Chancen haben.
\newline
\newline
Im zweiten Experiment werden den Agenten feste Bearbeitungskosten zugewiesen: $[2, 3, 5, 7, 8]$ (siehe Kapitel \ref{price}). Diese Werte decken den vollständigen Zustandsraum des Modells ab. Der daraus resultierende durchschnittliche Auftragswert liegt bei 5, sodass in jeder Runde Agenten mit Gewinn, Break-Even und Verlust gegenüber ihren individuellen Kosten konfrontiert sind.

\subsection{Analyse des Lernverhaltens (Forschungsfrage 1)}
Die erste Forschungsfrage untersucht, ob Agenten im dynamischen Marktumfeld durch wiederholte Teilnahme stabile und vorteilhafte Strategien entwickeln. Dazu wird das Verhalten der Agenten im Szenario mit dynamischen Kosten analysiert, bei dem jeder Agent in jeder Runde neue Bearbeitungskosten aus einer Normalverteilung zieht ($\mu_{cost} = 5$, $\sigma_{cost} = 2$).
\newline
\begin{figure}
    \centering
    \includegraphics[width=1.0\textwidth]{figures/q_table_rz5.png}
    \caption{Exemplarische Q-Tabelle von Agent RZ5 nach 8000 Runden.}
    \label{fig:q_table_rz5}
\end{figure}
Ein zentrales Element dieser Analyse ist die exemplarische Q-Tabelle von Agent RZ5 (siehe Abbildung \ref{fig:q_table_rz5}). Sie zeigt, dass der Agent in Zuständen mit Verlusten systematisch die Aktion „Job inserieren“ wählt, unabhängig vom vorherigen Marktzustand. In Situationen mit Gewinn hingegen bevorzugt er das Bieten mit einem moderaten Faktor (0.5 des Gewinn aus der Selbstbearbeitung). Dieses Verhalten ist konsistent mit einer ökonomisch rationalen Strategie: Der Agent vermeidet risikobehaftete Aktionen bei Verlust und nutzt günstige Kostenlagen gezielt aus.
\newline
\begin{figure}[!htb]
    \begin{subfigure}{0.5\textwidth}
        \centering
        \includegraphics[width=0.9\linewidth]{figures/q_table_convergence.png}
        \caption{Entwicklung der durchschnittlichen Änderung der Q-Tabelle.}
        \label{fig:q_table_rz3}
    \end{subfigure}
    \begin{subfigure}{0.5\textwidth}
        \centering
        \includegraphics[width=0.9\linewidth]{figures/social_welfare.png}
        \caption{Entwicklung des Social Welfare über die Zeit.}
        \label{fig:social_welfare}
    \end{subfigure}
\end{figure}
Die Entwicklung der Q-Werte über die Zeit wird in Abbildung \ref{fig:q_table_rz3} dargestellt. Hier zeigt sich, dass sich die Q-Werte aller Agenten im Laufe der Simulation deutlich stabilisieren. Bereits nach etwa 3000 Runden geht die durchschnittliche Änderung der Q-Tabelle deutlich zurück und pendelt sich auf einem niedrigen Niveau ein. Dies deutet auf eine Konvergenz der individuellen Lernprozesse hin, die Agenten haben stabile Strategien ausgebildet.
\newline
Auch die Entwicklung des Social Welfare über die Zeit bestätigt diesen Trend (siehe Abbildung \ref{fig:social_welfare}). Nach anfänglichen Schwankungen steigt der durchschnittliche Gesamtertrag des Systems kontinuierlich an und erreicht ab Runde 3000 ein weitgehend stabiles Niveau. Dies legt nahe, dass nicht nur individuelle Agenten voneinander lernen, sondern dass sich auch das Zusammenspiel im Markt verbessert.
\newline
Insgesamt lässt sich festhalten, dass Agenten in der Lage sind, unter dynamischen Bedingungen durch Q-Learning vorteilhafte Strategien zu entwickeln. Diese Strategien stabilisieren sich im Zeitverlauf, führen zu verbesserten individuellen Entscheidungen und zu einer kollektiv effizienteren Ressourcennutzung.

\subsection{Analyse der Ergebnisverteilung (Forschungsfrage 2)}
Die zweite Forschungsfrage untersucht, ob sich Gewinne und Aufträge im Markt langfristig gleichmäßig auf die Agenten verteilen oder ob sich einzelne Agenten als dauerhafte Gewinner durchsetzen. Zur Beantwortung wird das Experiment mit statischen Agentenkosten herangezogen. Die konstanten Kostenverhältnisse erlauben es, strukturelle Vorteile einzelner Agenten sichtbar zu machen.
\newline
\begin{figure}
    \centering
    \includegraphics[width=1.0\textwidth]{figures/reward_agents.png}
    \caption{Kumulative Gewinne der Agenten.}
    \label{fig:reward_agents}
\end{figure}
Die Auswertung der kumulierten Gewinne (siehe Abbildung \ref{fig:reward_agents}) zeigt deutliche Unterschiede. Agent RZ1 erzielt den höchsten Gesamtertrag mit einem durchschnittlichen Reward von 4.90, während Agent RZ2 trotz ähnlich niedriger Kosten mit nur 3.14 deutlich schlechter abschneidet. Die übrigen Agenten bewegen sich im Mittelfeld. Besonders auffällig ist, dass RZ3 mit Break-Even-Kosten einen höheren Gesamtertrag erzielt als RZ2, obwohl dieser strukturell besser positioniert ist, genauso wie die Agenten RZ4 und RZ5.
\newline
\begin{figure}[!htb]
    \begin{subfigure}{0.5\textwidth}
        \centering
        \includegraphics[width=0.9\linewidth]{figures/processed_jobs.png}
        \caption{Bearbeitete Aufträge.}
        \label{fig:processed_jobs}
    \end{subfigure}
        \begin{subfigure}{0.5\textwidth}
        \centering
        \includegraphics[width=0.9\linewidth]{figures/processed_vs_listed.png}
        \caption{Gelistete vs. bearbeitete Aufträge.}
        \label{fig:processed_vs_listed}
    \end{subfigure}
\end{figure}
Die Analyse der bearbeiteten Aufträge (siehe Abbildung \ref{fig:processed_jobs}) unterstreicht diese Unterschiede. RZ1 und RZ2 bearbeiten nicht nur durchgehend ihre eigenen Jobs, sondern gewinnen auch zahlreiche zusätzliche Aufträge am Markt (7.538 bzw. 7.378). Die übrigen Agenten bearbeiten ausschließlich ihre eigenen Aufträge und gewinnen keine fremde. Diese Verteilung zeigt, dass nur Agenten mit Gewinn tatsächlich am Markt agieren.
\newline
Ein weiteres Indiz bietet die Strategieauswertung (siehe Abbildung \ref{fig:processed_vs_listed}). RZ1 und RZ2 listen ihre Aufträge kaum am Markt, sondern bearbeiten sie nahezu vollständig selbst. Für sie besteht kein Anreiz zur Auslagerung, da ihre geringen Kosten eine Selbstbearbeitung besonders profitabel machen. Die übrigen Agenten hingegen listen ihre Aufträge in einem Großteil der Runden, was ihnen bei hoher Konkurrenz nur selten Vorteile verschafft.
\newline
\begin{table}[htb]
    \centering
    \begin{subtable}{0.48\textwidth}
        \centering
        \begin{tabular}{llll}
            \multicolumn{1}{l|}{RZ} & Mean & Stdabw. & Std/Mean \\ \hline
            \multicolumn{1}{l|}{RZ1} & 4.90 & 0.70    & 0.1427   \\
            \multicolumn{1}{l|}{RZ2} & 3.14 & 0.46    & 0.1476   \\
            \multicolumn{1}{l|}{RZ3} & 4.30 & 4.18    & 0.9734   \\
            \multicolumn{1}{l|}{RZ4} & 3.73 & 4.18    & 1.1198   \\
            \multicolumn{1}{l|}{RZ5} & 3.42 & 4.29    & 1.2553  
        \end{tabular}
        \caption{Statisches Szenario.}
        \label{tab:static_costs}
    \end{subtable}
    \hfill
    \begin{subtable}{0.48\textwidth}
        \centering
        \begin{tabular}{llll}
            \multicolumn{1}{l|}{RZ}  & Mean & Stdabw. & Std/Mean \\ \hline
            \multicolumn{1}{l|}{RZ1} & 3.37 & 3.49    & 1.0362   \\
            \multicolumn{1}{l|}{RZ2} & 3.41 & 3.42    & 1.0030   \\
            \multicolumn{1}{l|}{RZ3} & 3.44 & 3.39    & 0.9863   \\
            \multicolumn{1}{l|}{RZ4} & 3.36 & 3.34    & 0.9941   \\
            \multicolumn{1}{l|}{RZ5} & 3.46 & 3.38    & 0.9767  
        \end{tabular}
        \caption{Dynamisches Szenario.}
        \label{tab:dynamic_costs}
    \end{subtable}
    \caption{Statistische Kennzahlen der durchschnittlichen Rewards je Agent.}
    \label{tab:rewards_stats}
\end{table}
Die statistischen Kennzahlen (Tabelle \ref{tab:static_costs}) stützen diese Beobachtungen, während RZ1 und RZ2 mit niedriger Standardabweichung und geringem $\frac{\sigma}{\mu}$-Verhältnis sehr stabile Erträge erzielen, schwanken die Rewards von RZ3 bis RZ5 stark. Ein Hinweis auf instabile Erträge und wenige gesicherte Gewinnstrategien. Die Agenten mit hohen Fixkosten haben langfristig schlechtere Chancen, sich am Markt zu behaupten.
\newline
Bemerkenswert ist, dass RZ2 trotz strukturellem Vorteil nicht vom System profitiert. Dies deutet darauf hin, dass langfristiger Erfolg im Markt nicht allein von der Kostenstruktur abhängt, sondern stark vom erlernten Verhalten des Agenten beeinflusst wird. Ungünstige Strategien oder ineffizientes Bietverhalten können dazu führen, dass selbst gut positionierte Agenten zurückfallen.
\newline
Ein anderer Befund zeigt sich im dynamischen Szenario mit zufällig gezogenen Kosten (siehe Tabelle \ref{tab:dynamic_costs}). Hier gleichen sich langfristig sowohl die durchschnittlichen Rewards als auch deren Schwankungen zwischen den Agenten weitgehend an. Alle Agenten erreichen ähnlich hohe Mittelwerte bei annähernd gleicher Streuung ($\frac{\sigma}{\mu} \approx 1$). Dies spricht für eine gerechtere Verteilung und zeigt, dass bei hoher Unsicherheit strukturelle Vorteile reduziert werden und sich keine klaren Gewinner etablieren.
\newline
Insgesamt zeigen die Ergebnisse, dass sich bei festen Kostenverhältnissen Agenten mit günstiger Kostenstruktur häufiger durchsetzen, vorausgesetzt, sie entwickeln auch effektive Strategien. Lernen und Strategieentwicklung spielen eine zentrale Rolle für den langfristigen Markterfolg. In dynamischeren Umgebungen hingegen nivellieren sich strukturelle Vorteile weitgehend, was zu einer gleichmäßigeren Ergebnisverteilung führt.

\section{Fazit und Ausblick}
Die vorliegende Arbeit untersucht, wie sich Agenten im dezentralen Markt für Auftragsvergabe mithilfe von Q-Learning verhalten und welche Verteilungsmuster sich über Zeit hinweg herausbilden. Die Experimente zeigen, dass Agenten unter dynamischen Marktbedingungen in der Lage sind, stabile und ökonomisch sinnvolle Strategien zu entwickeln. Diese Strategien führen nicht nur zu stabileren individuellen Entscheidungen, sondern auch zu einem ansteigenden Social Welfare im Gesamtsystem.
 \newline
Im statischen Szenario mit festen Agentenkosten hingegen treten deutliche Unterschiede zwischen den Agenten zutage. RZs mit strukturell günstigen Bedingungen erzielen nicht automatisch den höchsten Gewinn, vielmehr ist der Erfolg stark vom erlernten Verhalten abhängig. Damit zeigt sich, dass Lernfähigkeit und Strategiefindung im Markt eine zentrale Rolle für den langfristigen Erfolg spielen.
\newline
Für zukünftige Arbeiten ergeben sich mehrere sinnvolle Erweiterungen. Erstens könnte das aktuelle Auktionsverfahren durch realistischere Mechanismen ersetzt werden, etwa durch Einführung eines Mindestgebots oder die Anwendung des Vickrey-Verfahrens, bei dem das zweithöchste Gebot den Zuschlagspreis bestimmt. Dies würde die strategische Tiefe erhöhen und könnte das Verhalten der Agenten nochmals deutlich verändern.
\newline
Zweitens könnte die Knappheit von Aufträgen erhöht werden, etwa durch eine Reduktion der zu vergebenden Jobs pro Runde oder durch das Einführen eines externen Faktors, der den Marktpreis beeinflusst. Ein solches Szenario würde die Wettbewerbssituation verschärfen und den Druck auf die Agenten erhöhen, was zu komplexeren Lernverläufen und dynamischeren Strategien führen dürfte.

% ****************************************************************************
% BIBLIOGRAPHY AREA
% ****************************************************************************
\begin{footnotesize}

\bibliographystyle{unsrt}
\bibliography{bibliography.bib}

\end{footnotesize}
% ****************************************************************************
% END OF BIBLIOGRAPHY AREA
% ****************************************************************************

\end{document}

