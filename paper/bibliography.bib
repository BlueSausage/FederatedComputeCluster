@misc{FaramaFoundation,
  title = {Farama {{Foundation}}},
  year = {2025},
  journal = {The Farama Foundation},
  urldate = {2025-03-08},
  abstract = {Maintaining The World's Open Source Reinforcement Learning Tools},
  howpublished = {https://farama.org/about},
  langid = {english},
  file = {/Users/niklasschneider/Zotero/storage/9EJZ8XIC/about.html}
}

@article{gadgil,
  title = {Solving {{The Lunar Lander Problem}} under {{Uncertainty}} Using {{Reinforcement Learning}}},
  author = {Gadgil, Soham and Xin, Yunfeng and Xu, Chengzhe},
  year = {2020},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2011.11850},
  urldate = {2025-02-16},
  abstract = {Reinforcement Learning (RL) is an area of machine learning concerned with enabling an agent to navigate an environment with uncertainty in order to maximize some notion of cumulative long-term reward. In this paper, we implement and analyze two different RL techniques, Sarsa and Deep QLearning, on OpenAI Gym's LunarLander-v2 environment. We then introduce additional uncertainty to the original problem to test the robustness of the mentioned techniques. With our best models, we are able to achieve average rewards of 170+ with the Sarsa agent and 200+ with the Deep Q-Learning agent on the original problem. We also show that these techniques are able to overcome the additional uncertainities and achieve positive average rewards of 100+ with both agents. We then perform a comparative analysis of the two techniques to conclude which agent peforms better.},
  copyright = {Creative Commons Attribution 4.0 International},
  keywords = {FOS: Computer and information sciences,Machine Learning (cs.LG)}
}

@inproceedings{guttulsrud,
  title = {Solving the {{Lunar Lander Problem}} with {{Multiple Uncertainties}} Using a {{Deep Q-Learning}} Based {{Short-Term Memory Agent}}},
  booktitle = {2023 12th {{International Conference}} on {{Computing}} and {{Pattern Recognition}}},
  author = {Guttulsrud, H{\aa}kon and Sandnes, Mathias and Shrestha, Raju},
  year = {2023},
  month = oct,
  pages = {27--33},
  publisher = {ACM},
  address = {Qingdao China},
  doi = {10.1145/3633637.3633641},
  urldate = {2025-02-16},
  isbn = {9798400707988},
  langid = {english},
  file = {/Users/niklasschneider/Zotero/storage/UQFZR9RL/Guttulsrud et al. - 2023 - Solving the Lunar Lander Problem with Multiple Unc.pdf}
}

@misc{LunarLanderGymnasium,
  title = {Lunar {{Lander Gymnasium Documentation}}},
  year = {2024},
  urldate = {2025-02-16},
  abstract = {A standard API for reinforcement learning and a diverse set of reference environments (formerly Gym)},
  howpublished = {https://gymnasium.farama.org/environments-/box2d/lunar\_lander.html},
  langid = {english},
  file = {/Users/niklasschneider/Zotero/storage/YT53HP5G/lunar_lander.html}
}

@article{mnih2015,
  title = {Human-Level Control through Deep Reinforcement Learning},
  author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A. and Veness, Joel and Bellemare, Marc G. and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K. and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
  year = {2015},
  month = feb,
  journal = {Nature},
  volume = {518},
  number = {7540},
  pages = {529--533},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/nature14236},
  urldate = {2025-02-16},
  langid = {english}
}

@misc{mnih2013,
  title = {Playing {{Atari}} with {{Deep Reinforcement Learning}}},
  author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
  year = {2013},
  month = dec,
  number = {arXiv:1312.5602},
  eprint = {1312.5602},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1312.5602},
  urldate = {2025-02-16},
  abstract = {We present the first deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning. The model is a convolutional neural network, trained with a variant of Q-learning, whose input is raw pixels and whose output is a value function estimating future rewards. We apply our method to seven Atari 2600 games from the Arcade Learning Environment, with no adjustment of the architecture or learning algorithm. We find that it outperforms all previous approaches on six of the games and surpasses a human expert on three of them.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/niklasschneider/Zotero/storage/CC28CUIX/Mnih et al. - 2013 - Playing Atari with Deep Reinforcement Learning.pdf;/Users/niklasschneider/Zotero/storage/9X8QDM82/1312.html}
}

@inproceedings{nugroho,
  title = {Powered {{Landing Guidance Algorithms Using Reinforcement Learning Methods}} for {{Lunar Lander Case}}},
  author = {Nugroho, Larasmoyo and Rahma Zani, Novanna and Qomariyah, Nurul and Akmeliawati, Rini and Andiarti, Rika and Kusuma Wijaya, Sastra},
  year = {2021},
  month = jun,
  doi = {10.30536/j.jtd.2021.v19.a3573}
}

@incollection{pareigis,
  title = {Algorithmic {{Foundations}} of {{Reinforcement Learning}}},
  booktitle = {Advances in {{Real-Time}} and {{Autonomous Systems}}},
  author = {Pareigis, Stephan},
  editor = {Unger, Herwig and Schaible, Marcel},
  year = {2024},
  volume = {1009},
  pages = {1--27},
  publisher = {Springer Nature Switzerland},
  address = {Cham},
  doi = {10.1007/978-3-031-61418-7_1},
  urldate = {2025-03-16},
  isbn = {978-3-031-61417-0 978-3-031-61418-7},
  langid = {english}
}

@misc{remman,
  title = {Deep {{Reinforcement Learning Behavioral Mode Switching Using Optimal Control Based}} on a {{Latent Space Objective}}},
  author = {Remman, Sindre Benjamin and Kristiansen, Bj{\o}rn Andreas and Lekkas, Anastasios M.},
  year = {2024},
  month = jun,
  number = {arXiv:2406.01178},
  eprint = {2406.01178},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2406.01178},
  urldate = {2025-02-16},
  abstract = {In this work, we use optimal control to change the behavior of a deep reinforcement learning policy by optimizing directly in the policy's latent space. We hypothesize that distinct behavioral patterns, termed behavioral modes, can be identified within certain regions of a deep reinforcement learning policy's latent space, meaning that specific actions or strategies are preferred within these regions. We identify these behavioral modes using latent space dimension-reduction with {\textbackslash}ac*\{pacmap\}. Using the actions generated by the optimal control procedure, we move the system from one behavioral mode to another. We subsequently utilize these actions as a filter for interpreting the neural network policy. The results show that this approach can impose desired behavioral modes in the policy, demonstrated by showing how a failed episode can be made successful and vice versa using the lunar lander reinforcement learning environment.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Systems and Control,Electrical Engineering and Systems Science - Systems and Control},
  file = {/Users/niklasschneider/Zotero/storage/ZVAMKMZ3/Remman et al. - 2024 - Deep Reinforcement Learning Behavioral Mode Switch.pdf;/Users/niklasschneider/Zotero/storage/P2KMQ2FR/2406.html}
}

@inproceedings{sadavarte,
  title = {Solving the {{Lunar Lander Problem}} Using {{Reinforcement Learning}}},
  booktitle = {2021 {{IEEE International Conference}} on {{Computation System}} and {{Information Technology}} for {{Sustainable Solutions}} ({{CSITSS}})},
  author = {Sadavarte, Rohit Sachin and Raj, Rishab and Sathish Babu, B},
  year = {2021},
  month = dec,
  pages = {1--6},
  publisher = {IEEE},
  address = {Bangalore, India},
  doi = {10.1109/CSITSS54238.2021.9682970},
  urldate = {2025-02-16},
  copyright = {https://doi.org/10.15223/policy-029},
  isbn = {978-1-66540-610-9}
}

@inproceedings{shah,
  title = {Deep {{Reinforcement Learning}} for {{Unpredictability-Induced Rewards}} to {{Handle Spacecraft Landing}}},
  booktitle = {2023 13th {{International Conference}} on {{Information Science}} and {{Technology}} ({{ICIST}})},
  author = {Shah, Salman and Yao, Nianmin},
  year = {2023},
  month = dec,
  pages = {73--79},
  publisher = {IEEE},
  address = {Cairo, Egypt},
  doi = {10.1109/ICIST59754.2023.10367162},
  urldate = {2025-02-16},
  copyright = {https://doi.org/10.15223/policy-029},
  isbn = {9798350313925}
}

@inproceedings{shen,
  title = {Comparison of {{Three Deep Reinforcement Learning Algorithms}} for {{Solving}} the {{Lunar Lander Problem}}},
  booktitle = {2023 {{International Conference}} on {{Data Science}}, {{Advanced Algorithm}} and {{Intelligent Computing}} ({{DAI}} 2023)},
  author = {Shen, Dingli},
  year = {2024},
  pages = {187--199},
  publisher = {Atlantis Press},
  isbn = {94-6463-370-0}
}

@book{sutton,
  title = {Reinforcement Learning: An Introduction},
  shorttitle = {Reinforcement Learning},
  author = {Sutton, Richard S. and Barto, Andrew G.},
  year = {2018},
  series = {Adaptive Computation and Machine Learning Series},
  edition = {Second edition},
  publisher = {The MIT Press},
  address = {Cambridge, Massachusetts},
  abstract = {"Reinforcement learning, one of the most active research areas in artificial intelligence, is a computational approach to learning whereby an agent tries to maximize the total amount of reward it receives while interacting with a complex, uncertain environment. In Reinforcement Learning, Richard Sutton and Andrew Barto provide a clear and simple account of the field's key ideas and algorithms."--},
  isbn = {978-0-262-03924-6},
  lccn = {Q325.6 .R45 2018},
  keywords = {Reinforcement learning}
}
