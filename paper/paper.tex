\documentclass{sop}
\usepackage[ngerman]{babel}
\usepackage{lipsum}
\usepackage{amsmath,amsfonts,amssymb}
\usepackage{tikz, graphics, subcaption}
\usepackage{tcolorbox}
\usepackage{graphicx}
\usepackage{fancyhdr}
\usepackage{hyperref}

\voffset 0 cm \hoffset 0 cm \addtolength{\textwidth}{0cm}
\addtolength{\textheight}{0cm}\addtolength{\leftmargin}{0cm}

\begin{document}

\title{Lernverhalten und Gewinnverteilung im Multi-Agenten-Markt für Auftragsvergabe mit Q-Learning}

%***********************************************************************
% AUTHORS INFORMATION AREA
%***********************************************************************
\author{Niklas Schneider
%
% DO NOT MODIFY THE FOLLOWING '\vspace' ARGUMENT
\vspace{.3cm}\\
%
HAW Hamburg - Department Informatik \\
Berliner Tor 5, 20099 Hamburg \\
niklas.schneider@haw-hamburg.de
}
%***********************************************************************
% END OF AUTHORS INFORMATION AREA
%***********************************************************************

\maketitle

\begin{abstract}
\lipsum[1]
\end{abstract}

\section{Einführung}
Im Rahmen des Praktikum zu verteilten adaptiven Systemen wurde die Auftragsvergabe zwischen Rechenzentren (RZs) bisher zentral oder regelbasiert organisiert. In dieser Arbeit wird das Szenario zu einem dezentralen Markt weiterentwickelt, in dem mehrere RZs als autonome, lernende Agenten agieren. Sie können Aufträge selbstbearbeiten, auf dem Markt anbieten oder mit Erlösen aus der Selbstbearbeitung auf fremde Aufträge bieten. Ziel ist es, mit Hilfe von Q-Learning zu untersuchen, wie sich Strategien, Gewinne und die Verteilung der Aufträge unter realistischen Marktbedingungen entwickeln.

\subsection{Preisgestaltung}
Sowohl die Kosten der RZs als auch der Auftragswert für Aufträge werden in jeder Runde dynamisch generiert, um ein realistisches, variierendes Marktumfeld zu simulieren.
\newline
Jedes RZ zieht zu beginn jeder Runde seine individuellen Bearbeitungskosten aus einer Normalverteilung $N(\mu_{cost}, \sigma_{cost})$, mit einem Mittelwert $\mu_{cost}$ von 5 und einer Standardabweichung $\sigma_{cost}$ von 2. Die gezogenen Werte werden auf positive ganze Zahlen abgerundet und auf einen Wertebreich von 1 bis 10 begrenzt. Dadurch wird das einteilen der Kosten in diskrete Kategorien erleichtert.
\newline
Der Preis, der in einer Runde für alle Aufträge gleichermaßen gilt, wird als der Mittelwert der gezogenen Kosten aller RZs berechnet. Dieser Wert wird auf eine positive ganze Zahl aufgerundet, sodass der Auftragswert immer so hoch wie der Durchschnitt der Bearbeitungskosten aller RZs ist. Dies stellt sicher, dass es für die RZs grundsätzlich möglich ist, durch Selbstbearbeitung Gewinne zu erzielen, während gleichzeitig ein Wettbewerbsdruck durch die variierenden Kosten entsteht.
\newline
Durch die Kombination aus zufälligen, eingegrenzten Kosten und rundenweise neu berechnetem Preis kann es in einzelnen Runden zu Verlusten bei der Selbstbearbeitung kommen. RZs mit überdurchschnittlich Kosten können dann versuchen, ihren Auftrag über den Markt zu verkaufen. Auf diese Weise entsteht ein lernrelevantes Spannungsfeld zwischen Selbstbearbeitung, Outsourcing und Marktteilnahme.

\subsection{Auktionsverfahren}
In dem betrachteten Marktszenario haben RZs in jeder Runde drei Handlungs-möglichkeiten. Sie können ihren Auftrag selbst bearbeiten, ihn auf dem Markt zum Verkauf anbieten oder mit dem Erlös aus der Selbstbearbeitung ein Gebot für einen weiteren Auftrag abgeben.
\newline
Wird ein Auftrag auf dem Markt angeboten, so erfolgt dies ohne eines Mindestgebots. Alle angeboten Aufträge werden gleich behandelt und ohne individuelle Preisuntergrenze gelistet. Die RZs, die ein Gebot abgeben möchten, wählen dazu einen Betrag aus, den sie bereit sind, für die Bearbeitung eines fremden Auftrags zu zahlen. Die Höhe des Gebots orientiert sich an dem Gewinn aus der Selbstbearbeitung in der laufenden Runde, wobei feste Gebotsfaktoren verwendet werden (vgl Abschnitt \ref{actionspace} Aktionsraum).
\newline
Nachdem alle RZs ihre Entscheidung getroffen haben, erfolgt das Matching der Marktangebote. Die auf dem Markt gelisteten Aufträge werden zufällig einem der Bieter zugewiesen, wobei in absteigender Reihenfolge der Gebote vorgegangen wird. Jedes Angebot kann dabei maximal einmal vergeben werden. Es ist somit möglich, dass RZs, die ihren Auftrag auf dem Markt anbieten, keinen Käufer finden. Ebenso konnen RZs, die ein Gebot abgeben, leer ausgehen, wenn kein passender Auftrag mehr verfügbar ist.
\newline
Dieses einfache, nicht-kooperative Marktverfahren erschafft einen Auktionsmechanismus mit begrenztem Wettbewerb und Unsicherheit, der des den Agenten erlaubt, durch wiederholte Teilnahme und Lernen über Q-Learning geeignete Strategien zu entwickeln.

\section{Forschungsfrage}
Im Zentrum der Arbeit steht die Frage, wie sich das Verhalten und die Ergebnisse im beschriebenen Marktszenario entwickeln, wenn mehrere RZs als autonome Agenten mithilfe von Q-Learning agieren. Dabei werden insbesondere die Stabilität der Strategien, die Herausbildung von Gewinnern und die Verteilung der Aufträge analysiert.
\\
Daraus ergeben sich die folgenden Forschungsfragen:
\begin{enumerate}
    \item Entwickeln Agenten im Multi-Agenten-Markt für Auftragsvergabe durch wiederholte Teilnahme stabile und vorteilhafte Strategien?
    \item Verteilen sich Gewinne und Aufträge langfristig gleichmäßig, oder setzen sich einzelne Agenten als Gewinner durch?
    \item Wie beeinflusst die Kostenstruktur (fix vs. dynamisch) das Lernverhalten der Agenten und die Stabilität sowie Fairness des Marktsystems
\end{enumerate}
Evtll. können weitere Fragen ergänzt werden, je nach Fokus der Arbeit.
\begin{enumerate}
    \item Führt eigennütziges, lernendes Verhalten autonomer RZs langfristig zu stabilen Strategien – und wie wirkt sich das auf Marktverteilung und Social Welfare aus?
\end{enumerate}

Um diese Fragen zu beantwortung wird eine simulationsbasierte Analyse durchgeführt. Dazu wird eine Multi-Agenten-Simulation entwickelt, in der mehrere RZs als autonome Agenten mithilfe von Q-Learning im Markt agieren. Die Agenten verfügen dabei jeweils nur über Informationen zu ihren eigenen Kosten und zur Gebühr für die Bearbeitung eines Auftrags, sowie die Marktauslastung der letzten Runde. Untersucht werden sowohl Szenarien mit festen Kosten je Agent als auch mit dynamischen, pro Runde neu gezogenen Kosten. Dieses Vorgehen ermöglicht es, das Zusammenspiel von individuellen Lernprozessen, Strategieentwicklung und Systemdynamik im dezentralen Markt direkt zu beobachten. Analytische Lösungen sind bei komplexem Agentenverhalten und Lernalgorithmen oft nicht praktikabel. Daher bietet die simulationsbasierte Auswertung eine realistische Möglichkeit, die Forschungsfragen systematisch zu untersuchen.

\section{Methodik/Grundlagen}
\subsection{Q-Learning}
Q-Learning ist ein modellfreies Verfahren des Reinforcement Learning, das darauf abzielt, eine optimale Strategie (Policy) $\pi^*$ für einen Agenten zu erlernen. Ziel ist es, für jeden Zustand $s$ eine Aktion $a$ zu finden, die langfristig die höchste kumulierte Belohnung verspricht. Dabei erhält der Agent bei der Ausführung von Aktion $a$ in Zustand $s$ eine direkte Belohnung $r(s, a)$ und lernt aus der Übergangsdynamik der Umgebung.
Zentral ist die sogenannte Q-Funktion $Q(s, a)$ die den erwarteten Nutzen angibt, wenn der Agent in Zustand $s$ die Aktion $a$ wählt und danach der optimalen Policy $\pi^*$ folgt. Die optimale Q-Funktion $Q^*$ erfüllt die Bellmann-Gleichung:
\[
Q^*(s, a) = r(s, a) + \gamma \cdot \sum_{s'} p(s'|s, a) \cdot \max_{a'} Q^*(s', a')
\]
Da die Übergangswahrscheinlichkeit $p(s'|s, a)$ in modelfreien Verfahren nich bekannt sind, wird $Q(s, a)$ iterativ durch Interaktionen mit der Umgebung geschätzt. Die Aktualisierung der Q-Werte ist durch folgende Funktion definiert:
\[
Q(s, a) = Q(s, a) + \alpha \cdot (r(s, a) + \gamma \cdot \max_{a'}Q(s', a') - Q(s, a))
\]
Hierbei steht $\alpha$ für die Lernrate, die bestimmt, wie stark neue Informationen bestehende Schätzungen beeinflussen, und $\gamma$ ist der Diskontierungsfaktor, der den Einfluss zukünftiger Belohnungen gewichtet.
\newline
\newline
Um sowohl neue Strategien zu entdecken als auch bekannte gute Strategien auszunutzen, wird häufig eine $\epsilon$-greedy-Policy verwendet. Mit Wahrscheinlichkeit $\epsilon$ wählt der Agent eine zufällige Aktion (Exploration), andernfalls entscheidet sich der Agent mit der Wahrscheinlichleit $1-\epsilon$ für die aktuell beste bekannte Aktion (Exploitation). Dadurch wird eine angemessenes Maß an Exploration des Zustandsraums gewährleistet.

\subsection{Explodierende State und Action Spaces} \label{explodingstateactionspaces}
Beim Einsatz von Q-Learning in Multi-Agenten-Systemen führt die Kombination der Zustände und Aktionen aller Agenten zu einem exponentiell wachsenden Zustands- und Aktionsraum. Da Lernen im Kern als Suchprozess verstanden werden kann, wächst die Komplexität des Lernproblems mit der Anzahl beteiligter Variablen schnell an. Besonders im Multi-Agenten-Setting, in dem mehrere Agenten gleichzeitig agieren und interagieren, entstehen folgende Herausforderungen:
\begin{itemize}
    \item \textbf{Zustandsraum-Explosion:} Der Zustandsraum wächst exponentiell mit der Anzahl der Agenten und beobachtbaren Merkmale der Umgebung.
    \item \textbf{Aktionsraum-Explosion:} Ebenso wächst der gemeinsame Aktionsraum, da potenzielle Aktionskombinationen aller Agenten berücksichtigt werden müssen.
    \item \textbf{Ergebnisraum-Explosion:} Zusätzlich steigt die Zahl möglicher Ergebniszustände, also jener Zustände, die durch gleichzeitiges Handeln mehrerer Agenten erreicht werden können.
\end{itemize}
Diese Phänomene können dazu führen, dass klassische Lernverfahren wie Q-Learning schnell an praktische Grenzen stoßen, etwa, wenn der Speicher- oder Zeitaufwand für die Pflege der Q-Tabelle unhandhabbar wird. Um dieses Problem zu vermeiden, ist eine geeignete Reduktion und Diskretisierung der Zustände und Aktionen notwendig. \cite{tuylsmultiagent}

\subsection{Anwendung im Markt-Szenario}
Um das zuvor in \ref{explodingstateactionspaces} beschriebene Problem der Zustands- und Aktionsraumexplosion zu vermeiden, wurde das Markt-Szenario bewusst so modelliert, dass alle relevanten Elemente für tabellarisches Q-Learning diskret und handhabbar bleiben.
\newline
\newline
\textbf{Zustandsraum}\newline
Der Zustand eines Agenten setzt sich aus zwei diskretisierten Merkmalen zusammen:
\begin{enumerate}
    \item \textbf{Profitabilitätsniveau} - eine Einschätzung, ob der aktuelle Auftrag Verlust, Break-Even oder Gewinn bedeutet. Hierfür werden die individuell gezogenen Kosten mit dem aktuellen Auftragswert verglichen und in drei Kategorien eingeteilt:
    \begin{enumerate}
        \item[] 0: Verlust (Kosten $>$ Auftragswert)
        \item[] 1: Break-Even (Kosten $=$ Auftragswert)
        \item[] 2: Gewinn (Kosten $<$ Auftragswert)
    \end{enumerate}
    \item \textbf{Marktauslastung der Vorperiode} - eine Einschätzung der Wettbewerbssituation basierend auf der Anzahl der Maktangebote in der vorherigen Runde. Auch dieser Wert wird in drei diskrete Stufen eingeteilt:
    \begin{enumerate}
        \item[] 0: kein Angebot (leerer Markt)
        \item[] 1: vereinzelte Angebote (weniger Angebote als Agenten)
        \item[] 2: gesättigter Markt (genauso viele oder mehr Angebote als Agenten)
    \end{enumerate}
\end{enumerate}
Die Kombination dieser beiden Merkmale ergibt einen Zustandsraum von $3 \times 3 = 9$ diskreten Zuständen pro Agent.
\newline
\newline
\textbf{Aktionsraum} \label{actionspace}\newline
Um den Anforderungen von tabellarischem Q-Learning gerecht zu werden, wurde der Aktionsraum als festes, endliches Set diskreter Entscheidungen modelliert. Der Agent kann in jeder Runde aus folgenden sechs Aktionen wählen:
\begin{enumerate}
    \item [] Aktion 0: Eigenen Auftrag auf dem Markt anbieten
    \item [] Aktion 1: Auftrag selbst bearbeiten
    \item [] Aktion 2-5: Auf fremde Aufträge bieten, wobei das Gebot einem festen vielfachen $F \in \{0.25, 0.5, 0.75, 1.0\}$ des Erlöses entspricht, den der Agent durch die Selbstbearbeitung erzielen würde.
\end{enumerate}
Die Entscheidung zum Bieten wurde auf vier diskrete Bietstufen beschränkt, um den kontinuierlichen Aktionsraum kompakt und lernbar zu halten. Damit ergibt sich der folgende diskrete Aktionsraum:
\[
A = \{0, 1, 2, 3, 4, 5\}
\]
\newline
\textbf{Belohnungsstruktur}\newline
Die Belohnung eines Agenten basiert auf dem ökonomischen Ergebnis seiner Aktion in der jeweiligen Runde. Im Erfolgsfall entspricht die Belohnung dem tatsächlichen Gewinn, also dem Erlös aus dem erhaltenen Auftrag abzüglich der Bearbeitungskosten. Wird ein eigener Auftrag selbst bearbeitet oder erfolgreich auf dem Markt verkauft bzw. ersteigert und anschließend bearbeitet, ergibt sich eine positive Belohnung, sofern Gewinn erzielt wird.
\newline
Um jedoch unrealistisches Verhalten zu verhindern, erhält der Agent eine explizite Strafbelohnung von -10, wenn er eine Bietaktion (Aktion 2-5) auswählt, obwohl in dieser Runde kein Gewinn dich Selbstbearbeitung hätte erzielen können. Auf diese Weise wird vermieden, dass Agenten systematisch Gebote abgeben, obwohl ihnen dafür keine
wirtschaftliche Grundlage zur verfügung steht.
\newline
Nicht erfolgreiche Aktionen (z.B. kein Zuschlag bei Geboten oder keine Abnahme des angebotenen Aufztrags) führen zu einer Belohnung wie bei der Selbstbearbeitung.
\newline
Durch diese diskrete Modellierung ist es möglich, klassisches tabellarisches Q-Learning auch in einem dezentralen Multi-Agenten-Markt anzuwenden, ohne dass der Zustands- und Aktionsraum unhandhabbar groß wird.


\section{Experiment}
\begin{enumerate}
    \item Was mache ich um die Forschungsfragen zu beantworten. Szenarien, Parameter, Metriken
    \item Forschungsfragen beantworten
\end{enumerate}

\section{Fazit und Ausblick}
\dots

% ****************************************************************************
% BIBLIOGRAPHY AREA
% ****************************************************************************
\begin{footnotesize}

\bibliographystyle{unsrt}
\bibliography{bibliography.bib}

\end{footnotesize}
% ****************************************************************************
% END OF BIBLIOGRAPHY AREA
% ****************************************************************************

\end{document}

